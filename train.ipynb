{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from dataloader import *\n",
    "from RGCN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n"
     ]
    }
   ],
   "source": [
    "with open(\"DGL_graph.pkl\", \"rb\") as f:\n",
    "    g = pickle.load(f)\n",
    "    \n",
    "with open(\"data/conceptnet/embedding_values.pkl\", \"rb\") as f:\n",
    "    embedding_values = pickle.load(f)\n",
    "        \n",
    "g.ndata[\"x\"] = embedding_values\n",
    "\n",
    "# subsample a strongly-connected subgraph\n",
    "\n",
    "G_nx = g.to_networkx()\n",
    "sub_G_nx = nx.strongly_connected_components(G_nx)\n",
    "SCC = []\n",
    "for item in sub_G_nx:\n",
    "    if len(item) > 2:\n",
    "        SCC.append(item)\n",
    "component = list(SCC[0])\n",
    "\n",
    "# assign embedding to graph\n",
    "sub_graph = g.subgraph(component)\n",
    "sub_graph.copy_from_parent()\n",
    "sub_graph_G = sub_graph\n",
    "\n",
    "X,y = gen_training_set(sub_graph_G, 3, 100)\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configurations\n",
    "n_hidden = 16 # number of hidden units\n",
    "n_bases = -1 # use number of relations as number of bases\n",
    "n_hidden_layers = 0 # use 1 input layer, 1 output layer, no hidden layer\n",
    "n_epochs = 25 # epochs to train\n",
    "lr = 0.01 # learning rate\n",
    "l2norm = 0 # L2 norm coefficient\n",
    "\n",
    "n_input = 300\n",
    "n_output = 256\n",
    "num_rels = 34\n",
    "\n",
    "# create graph\n",
    "edge_norm = torch.ones(g.edata['rel_type'].shape[0])\n",
    "g.edata.update({'norm': edge_norm.view(-1,1)})\n",
    "\n",
    "# create model\n",
    "model = Model(n_input,\n",
    "              n_hidden,\n",
    "              n_output,\n",
    "              num_rels,\n",
    "              num_bases=n_bases,\n",
    "              num_hidden_layers=n_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.ndata['x'] = g.ndata['x'].float()\n",
    "output, g_emb = model(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graph_to_graph(nn.Module):\n",
    "    # take a Commonsense graph as input, output generated new nodes(phrase) and edges(phrase)\n",
    "    def __init__(self, input_size, hidden_size, node_output_size, phrase_output_size, edge_output_size, num_rels, n_hidden_layers, n_bases = -1):\n",
    "        super(graph_to_graph, self).__init__()\n",
    "        self.node = LSTM_node_generator(hidden_size, node_output_size)\n",
    "        self.phrase = LSTM_phrase_generator(hidden_size, phrase_output_size) #TODO: replace by gpt-2\n",
    "        self.edge = LSTM_edge_generator(hidden_size, edge_output_size)\n",
    "        # USE vanilla GCN\n",
    "        #self.graph_encoder = Net(input_size, 256, hidden_size)\n",
    "        # USE R-GCN\n",
    "        self.graph_encoder = Model(input_size,\n",
    "              hidden_size,\n",
    "              hidden_size,\n",
    "              num_rels,\n",
    "              num_bases=n_bases,\n",
    "              num_hidden_layers=n_hidden_layers)\n",
    "        \n",
    "    def generate_graph_embedding(self, g):\n",
    "        return self.graph_encoder(g)\n",
    "    \n",
    "    def node_policy(self, *args):\n",
    "        return self.node(*args)\n",
    "       \n",
    "    def generate_node_baseline(self, g):\n",
    "        node_embedding, g_embedding = self.graph_encoder(g)\n",
    "        node_embedding = list(node_embedding)\n",
    "        \n",
    "        node_decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        node_decoder_hidden = (g_embedding.view(1,1,-1), torch.zeros_like(g_embedding).view(1,1,-1))\n",
    "        \n",
    "        new_node_list = []\n",
    "        \n",
    "        # TODO: implementing teacher-forcing\n",
    "        for ni in range(max_length):\n",
    "            node_decoder_output, node_decoder_hidden = self.node(\n",
    "                node_decoder_input, node_decoder_hidden)\n",
    "            new_node_embedding = node_decoder_hidden\n",
    "            topv, topi = node_decoder_output.topk(1)\n",
    "            node_decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            print(node_decoder_input)\n",
    "            if node_decoder_input.item() == EOS_token: # stop generating node\n",
    "                break\n",
    "            else:  # new node embedding generated\n",
    "                new_node_list.append(node_decoder_input.item())\n",
    "        return new_node_list\n",
    "    \n",
    "    def generate_node(self, g):\n",
    "        node_embedding, g_embedding = self.graph_encoder(g)\n",
    "        node_embedding = list(node_embedding)\n",
    "        \n",
    "        node_decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        node_decoder_hidden = (g_embedding.view(1,1,-1), torch.zeros_like(g_embedding).view(1,1,-1))\n",
    "        \n",
    "        new_node_list = []\n",
    "        new_phrase_list = []\n",
    "        new_edge_list = []\n",
    "        \n",
    "        # TODO: implementing teacher-forcing\n",
    "        for ni in range(max_length):\n",
    "            node_decoder_output, node_decoder_hidden = self.node(\n",
    "                node_decoder_input, node_decoder_hidden)\n",
    "            new_node_embedding = node_decoder_hidden\n",
    "            topv, topi = node_decoder_output.topk(1)\n",
    "            node_decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            print(node_decoder_input)\n",
    "            if node_decoder_input.item() == EOS_token: # stop generating node\n",
    "                break\n",
    "            else:  # new node embedding generated\n",
    "                # add new node embedding to the list\n",
    "                new_node_list.append(new_node_embedding)\n",
    "                # generate new phrase\n",
    "                new_phrase = []\n",
    "                phrase_decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "                phrase_decoder_hidden = (new_node_embedding.view(1,1,-1), torch.zeros_like(new_node_embedding.view(1,1,-1)))\n",
    "                for pi in range(max_length):\n",
    "                    phrase_decoder_output, phrase_decoder_hidden = self.phrase(\n",
    "                        phrase_decoder_input, phrase_decoder_hidden)\n",
    "\n",
    "                    topv, topi = phrase_decoder_output.topk(1)\n",
    "                    phrase_decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                    if phrase_decoder_input.item() == EOS_token: # stop generating node\n",
    "                        break\n",
    "                    new_phrase.append(phrase_decoder_input)\n",
    "                    \n",
    "                new_phrase_list.append(new_phrase)\n",
    "                \n",
    "        return new_phrase_list\n",
    "    \n",
    "    def forward(self, g, max_length = 100):\n",
    "        # get graph embedding\n",
    "        node_embedding, g_embedding = self.graph_encoder(g)\n",
    "        node_embedding = list(node_embedding)\n",
    "        \n",
    "        node_decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        node_decoder_hidden = g_embedding\n",
    "        node_decoder_hidden = (g_embedding.view(1,1,-1), torch.zeros_like(g_embedding).view(1,1,-1))\n",
    "        \n",
    "        new_node_list = []\n",
    "        new_phrase_list = []\n",
    "        new_edge_list = []\n",
    "        \n",
    "        # TODO: implementing teacher-forcing\n",
    "        for ni in range(max_length):\n",
    "            node_decoder_output, node_decoder_hidden = self.node(\n",
    "                node_decoder_input, node_decoder_hidden)\n",
    "            new_node_embedding = node_decoder_hidden[0]\n",
    "            topv, topi = node_decoder_output.topk(1)\n",
    "            node_decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "            if node_decoder_input.item() == EOS_token: # stop generating node\n",
    "                break\n",
    "            else:  # new node embedding generated\n",
    "                # add new node embedding to the list\n",
    "                new_node_list.append(new_node_embedding)\n",
    "                # generate new phrase\n",
    "                new_phrase = []\n",
    "                phrase_decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "                for pi in range(max_length):\n",
    "                    phrase_decoder_output, phrase_decoder_hidden = self.phrase(\n",
    "                        phrase_decoder_input, phrase_decoder_hidden)\n",
    "\n",
    "                    topv, topi = phrase_decoder_output.topk(1)\n",
    "                    phrase_decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                    if phrase_decoder_input.item() == EOS_token: # stop generating node\n",
    "                        break\n",
    "                    new_phrase.append(phrase_decoder_input)\n",
    "                    \n",
    "                new_phrase_list.append(new_phrase)\n",
    "        # generate edge between nodes\n",
    "\n",
    "        for i, node1 in enumerate(node_embedding + new_node_list):\n",
    "            for j, node2 in enumerate(node_embedding + new_node_list):\n",
    "                edge_decoder_hidden = torch.cat([node1, node2])\n",
    "                edge_decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "                new_edge = []\n",
    "                for ei in range(max_length):\n",
    "                    edge_decoder_output, edge_decoder_hidden = self.edge(\n",
    "                    edge_decoder_input, edge_decoder_hidden)\n",
    "                    \n",
    "                    topv, topi = edge_decoder_output.topk(1)\n",
    "                    edge_decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                    if edge_decoder_input.item() == EOS_token: # stop generating node\n",
    "                        break\n",
    "                    new_edge.append(edge_decoder_input)\n",
    "                new_edge_list.append((i,j,new_edge))\n",
    "                \n",
    "        return new_phrase_list, new_edge_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from model import *\n",
    "CORPUS_SIZE = 10000\n",
    "input_size = 300\n",
    "hidden_size = 256\n",
    "node_output_size = g.ndata['x'].shape[0] + 2\n",
    "phrase_output_size = CORPUS_SIZE\n",
    "edge_output_size = CORPUS_SIZE\n",
    "num_rels = 34\n",
    "n_hidden_layers = 2\n",
    "n_bases = -1\n",
    "\n",
    "graph_generator = graph_to_graph(input_size, \n",
    "                                 hidden_size, \n",
    "                                 node_output_size, \n",
    "                                 phrase_output_size, \n",
    "                                 edge_output_size, \n",
    "                                 num_rels, \n",
    "                                 n_hidden_layers, \n",
    "                                 n_bases = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1785)\n",
      "tensor(1722)\n",
      "tensor(360)\n",
      "tensor(360)\n",
      "tensor(3174)\n",
      "tensor(3170)\n",
      "tensor(360)\n",
      "tensor(3170)\n",
      "tensor(600)\n",
      "tensor(4091)\n",
      "tensor(741)\n",
      "tensor(1920)\n",
      "tensor(531)\n",
      "tensor(1722)\n",
      "tensor(1577)\n",
      "tensor(600)\n",
      "tensor(636)\n",
      "tensor(1722)\n",
      "tensor(360)\n",
      "tensor(360)\n",
      "[1785, 1722, 360, 360, 3174, 3170, 360, 3170, 600, 4091, 741, 1920, 531, 1722, 1577, 600, 636, 1722, 360, 360]\n"
     ]
    }
   ],
   "source": [
    "graph_S = sub_graph_G.subgraph(X[0])\n",
    "graph_S.copy_from_parent()\n",
    "\n",
    "graph_S.ndata[\"x\"] = graph_S.ndata[\"x\"].float()\n",
    "edge_norm = torch.ones(graph_S.edata['rel_type'].shape[0])\n",
    "graph_S.edata.update({'norm': edge_norm.view(-1,1)})\n",
    "\n",
    "\n",
    "print(graph_generator.generate_node_baseline(graph_S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy gradient\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "gamma = 0.99\n",
    "class Policy(nn.Module):\n",
    "    # Wrap up the LSTM decisions\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        #self.state_space = env.observation_space.shape[0]\n",
    "        #self.action_space = env.action_space.n\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.graph_generator =  graph_to_graph(input_size, \n",
    "                                 hidden_size, \n",
    "                                 node_output_size, \n",
    "                                 phrase_output_size, \n",
    "                                 edge_output_size, \n",
    "                                 num_rels, \n",
    "                                 n_hidden_layers, \n",
    "                                 n_bases = -1)\n",
    "        \n",
    "        # Episode policy and reward history \n",
    "        self.policy_history = torch.Tensor([])\n",
    "        self.reward_episode = []\n",
    "        # Overall reward and loss history\n",
    "        self.reward_history = []\n",
    "        self.loss_history = []\n",
    "        self.action_history = []\n",
    "        \n",
    "    def forward(self, *args):    \n",
    "            return self.graph_generator.node_policy(*args)\n",
    "\n",
    "def select_action(policy, *args):\n",
    "    # state: (h_i, c_i), h_i ~ (1,1,hid_dim)\n",
    "    #Select an action (0 or 1) by running policy model and choosing based on the probabilities in state\n",
    "    d_action,state = policy(*args)\n",
    "    c = Categorical(torch.exp(d_action))\n",
    "    action = c.sample()\n",
    "\n",
    "    # Add log probability of our chosen action to our history    \n",
    "    if policy.policy_history.dim() != 0:\n",
    "        #print(policy.policy_history.shape, c.log_prob(action).shape)\n",
    "        policy.policy_history = torch.cat([policy.policy_history, c.log_prob(action).view(-1)])\n",
    "    else:\n",
    "        policy.policy_history = (c.log_prob(action))\n",
    "    return action, state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embedding, g_embedding = graph_generator.generate_graph_embedding(graph_S)\n",
    "\n",
    "node_decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "node_decoder_hidden = g_embedding\n",
    "node_decoder_hidden = (g_embedding.view(1,1,-1), torch.zeros_like(g_embedding).view(1,1,-1))\n",
    "\n",
    "output = select_action(policy, node_decoder_input, node_decoder_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy():\n",
    "    R = 0\n",
    "    rewards = []\n",
    "    \n",
    "    # Discount future rewards back to the present using gamma\n",
    "    for r in policy.reward_episode[::-1]:\n",
    "        R = r + policy.gamma * R\n",
    "        rewards.insert(0,R)\n",
    "        \n",
    "    # Scale rewards\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    rewards = (rewards - rewards.mean()) / (rewards.std() + np.finfo(np.float32).eps)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = (torch.sum(torch.mul(policy.policy_history, rewards).mul(-1), -1))\n",
    "    # Update network weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "    \n",
    "    #Save and intialize episode history counters\n",
    "    policy.loss_history.append(loss.item)\n",
    "    policy.reward_history.append(np.sum(policy.reward_episode))\n",
    "    policy.policy_history = torch.Tensor()\n",
    "    policy.reward_episode= []\n",
    "    policy.action_history = []\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "policy = Policy()\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "\n",
    "def main(episodes):\n",
    "    running_reward = 10\n",
    "    for episode in range(episodes):\n",
    "        #state = env.reset() # Reset environment and record the starting state\n",
    "        # initial state:\n",
    "        #policy = Policy(graph_generator)\n",
    "        node_embedding, g_embedding = policy.graph_generator.generate_graph_embedding(graph_S)\n",
    "        node_decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "        node_decoder_hidden = g_embedding\n",
    "        node_decoder_hidden = (g_embedding.view(1,1,-1), torch.zeros_like(g_embedding).view(1,1,-1))\n",
    "\n",
    "        for time in range(20):\n",
    "            action, state = select_action(policy, node_decoder_input, node_decoder_hidden)\n",
    "            policy.action_history.append(action.item())\n",
    "            # Step through environment using chosen action\n",
    "            #state, reward, done, _ = env.step(action.data[0])\n",
    "            reward = compute_reward(policy.action_history, y[0])\n",
    "            node_decoder_input = action\n",
    "            node_decoder_hidden = state\n",
    "            # Save reward\n",
    "            policy.reward_episode.append(reward)\n",
    "        \n",
    "        # Used to determine when the environment is solved.\n",
    "        running_reward = (running_reward * 0.99) + (time * 0.01)\n",
    "        action_history = policy.action_history\n",
    "        loss = update_policy()\n",
    "        if episode % 50 == 0:\n",
    "            print(action_history)\n",
    "            print('Episode {}\\tLast length: {:5d}\\tAverage length: {:.2f}\\t Loss: {:.4f}'.format(episode, time, running_reward, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(action_history, y):\n",
    "    #print(action_history, y)\n",
    "    score = len(set(y).intersection(set(action_history))) + (len(y) - len(action_history))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1080, 3082, 1357, 1286, 2101, 1352, 1175, 3810, 2327, 1877, 270, 303, 2152, 3673, 4340, 2607, 1121, 3521, 2029, 3440]\n",
      "Episode 0\tLast length:    19\tAverage length: 10.09\t Loss: 0.4051\n",
      "[2122, 1240, 4548, 956, 4011, 94, 1548, 4234, 3653, 2514, 2667, 966, 2773, 1767, 2260, 3834, 457, 289, 2679, 3701]\n",
      "Episode 50\tLast length:    19\tAverage length: 13.61\t Loss: -0.5620\n",
      "[4371, 3408, 3587, 45, 1524, 4312, 3763, 573, 828, 2633, 1960, 71, 1844, 535, 1505, 896, 4163, 1922, 3244, 1570]\n",
      "Episode 100\tLast length:    19\tAverage length: 15.74\t Loss: 7.3640\n",
      "[3162, 3729, 407, 2396, 4237, 4261, 1428, 3991, 2099, 1931, 215, 3982, 4438, 2708, 2875, 4238, 1807, 1843, 1975, 3993]\n",
      "Episode 150\tLast length:    19\tAverage length: 17.03\t Loss: 1.3405\n",
      "[4369, 4002, 2540, 873, 3285, 3056, 1304, 4332, 770, 2182, 3162, 1551, 2279, 837, 653, 2406, 2043, 1918, 2844, 4290]\n",
      "Episode 200\tLast length:    19\tAverage length: 17.81\t Loss: -4.3198\n",
      "[4036, 3813, 3271, 4389, 1847, 3993, 2456, 1287, 293, 2867, 3184, 1757, 6, 3818, 4544, 1093, 997, 1140, 347, 3244]\n",
      "Episode 250\tLast length:    19\tAverage length: 18.28\t Loss: -2.3851\n",
      "[1816, 918, 2676, 2985, 596, 1082, 487, 3391, 3351, 2459, 153, 2340, 1786, 4384, 2111, 962, 2095, 2067, 725, 201]\n",
      "Episode 300\tLast length:    19\tAverage length: 18.56\t Loss: 0.3125\n",
      "[3293, 1623, 4508, 740, 2713, 3871, 1438, 3108, 4277, 3634, 1380, 1250, 2227, 4366, 3836, 2275, 4284, 2616, 394, 3325]\n",
      "Episode 350\tLast length:    19\tAverage length: 18.74\t Loss: 11.6412\n",
      "[1825, 3996, 3964, 844, 3735, 2235, 1550, 3280, 155, 3, 1115, 863, 3745, 3521, 4284, 2264, 2793, 2018, 3501, 2350]\n",
      "Episode 400\tLast length:    19\tAverage length: 18.84\t Loss: -15.1465\n",
      "[1236, 3842, 1387, 3686, 3132, 4424, 2344, 2298, 3437, 4352, 2285, 4468, 2168, 3632, 2267, 3812, 1705, 1842, 1372, 179]\n",
      "Episode 450\tLast length:    19\tAverage length: 18.90\t Loss: 1.7319\n",
      "[335, 4530, 4236, 3088, 2014, 1269, 1862, 1567, 3854, 2588, 3002, 535, 846, 2959, 1171, 328, 1429, 171, 1171, 2844]\n",
      "Episode 500\tLast length:    19\tAverage length: 18.94\t Loss: -0.2013\n",
      "[3478, 1401, 151, 1044, 866, 1357, 2158, 1567, 1164, 45, 791, 3188, 692, 1816, 161, 3370, 512, 295, 535, 4502]\n",
      "Episode 550\tLast length:    19\tAverage length: 18.96\t Loss: 2.2567\n",
      "[1592, 1574, 3100, 4471, 4086, 2191, 1442, 915, 3346, 1576, 1679, 3324, 632, 4107, 1882, 2047, 4284, 4122, 3572, 250]\n",
      "Episode 600\tLast length:    19\tAverage length: 18.98\t Loss: -7.4417\n",
      "[3781, 3088, 2663, 3592, 3972, 2577, 188, 1948, 3350, 4173, 1767, 3324, 3900, 651, 116, 902, 3048, 4188, 3695, 1636]\n",
      "Episode 650\tLast length:    19\tAverage length: 18.99\t Loss: 5.1534\n",
      "[704, 2342, 419, 3758, 763, 3210, 2903, 990, 2849, 4528, 173, 4250, 3683, 2789, 2539, 907, 2350, 132, 2841, 3821]\n",
      "Episode 700\tLast length:    19\tAverage length: 18.99\t Loss: -7.2566\n",
      "[352, 1675, 2976, 3571, 1550, 2638, 3211, 3429, 1032, 516, 3159, 3505, 571, 1173, 2334, 2334, 3514, 2631, 1244, 615]\n",
      "Episode 750\tLast length:    19\tAverage length: 19.00\t Loss: -6.5529\n",
      "[3006, 3135, 1308, 4188, 1000, 4343, 3135, 1456, 167, 2334, 1637, 2757, 1890, 2713, 1725, 3758, 639, 652, 1256, 754]\n",
      "Episode 800\tLast length:    19\tAverage length: 19.00\t Loss: -10.3928\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-30674298a2b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-202-6cf5e8d8df5b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(episodes)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mrunning_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrunning_reward\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0maction_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-178-77a36b0a4a81>\u001b[0m in \u001b[0;36mupdate_policy\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Update network weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1061, 1827, 447]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
