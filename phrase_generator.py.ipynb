{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import numpy as np\n",
    "import pickle\n",
    "import networkx as nx\n",
    "from dataloader import *\n",
    "from RGCN import *\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- Finish loading -------------\n",
      "-------------- Training data subsampled -------------\n",
      "-------------- Training data generated -------------\n"
     ]
    }
   ],
   "source": [
    "with open(\"DGL_graph.pkl\", \"rb\") as f:\n",
    "    g = pickle.load(f)\n",
    "\n",
    "with open(\"data/conceptnet/embedding_values.pkl\", \"rb\") as f:\n",
    "    embedding_values = pickle.load(f)\n",
    "print(\"-------------- Finish loading -------------\")\n",
    "g.ndata[\"x\"] = embedding_values\n",
    "# subsample a strongly-connected subgraph\n",
    "\n",
    "G_nx = g.to_networkx()\n",
    "sub_G_nx = nx.strongly_connected_components(G_nx)\n",
    "SCC = []\n",
    "for item in sub_G_nx:\n",
    "    if len(item) > 2:\n",
    "        SCC.append(item)\n",
    "component = list(SCC[0])\n",
    "print(\"-------------- Training data subsampled -------------\")\n",
    "# assign embedding to graph\n",
    "sub_graph = g.subgraph(component)\n",
    "sub_graph.copy_from_parent()\n",
    "sub_graph_G = sub_graph\n",
    "\n",
    "#X,y = gen_training_set(sub_graph_G, 3, 1000)\n",
    "\n",
    "with open(\"training_data.pkl\", \"rb\") as f:\n",
    "    dic = pickle.load(f)\n",
    "X,y = dic[\"X\"], dic[\"y\"]\n",
    "print(\"-------------- Training data generated -------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer(\"data/conceptnet/embedding_keys.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/conceptnet/embedding_keys.pkl\", \"rb\") as f:\n",
    "    embedding_keys = pickle.load(f)\n",
    "for i,item in enumerate(embedding_keys):\n",
    "    words = item.replace(\"_\", \" \")\n",
    "    embedding_keys[i] = words\n",
    "embedding_keys = np.array(embedding_keys)\n",
    "index2phrase = embedding_keys[component]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_SIZE = len(tokenizer.vocab)\n",
    "input_size = 300\n",
    "hidden_size = 128\n",
    "node_output_size = 2\n",
    "phrase_output_size = CORPUS_SIZE\n",
    "num_rels = 34\n",
    "batch_size = 1\n",
    "num_batch = 2\n",
    "n_hidden_layers = 2\n",
    "n_bases = -1\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "node_generator = LSTM_node_generator(hidden_size, node_output_size, batch_size)\n",
    "graph_encoder = Model(input_size,\n",
    "            hidden_size,\n",
    "            hidden_size,\n",
    "            num_rels,\n",
    "            num_bases=n_bases,\n",
    "            num_hidden_layers=n_hidden_layers).to(device)\n",
    "phrase_generator = LSTM_phrase_generator(hidden_size, phrase_output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "graph_encoder.to(device)\n",
    "node_generator.to(device)\n",
    "phrase_generator.to(device)\n",
    "\n",
    "CLS_token = 0\n",
    "SEP_token = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_list = []\n",
    "y_list = []\n",
    "for i in range(100):\n",
    "    g = sub_graph_G.subgraph(X[i])\n",
    "    g.copy_from_parent()\n",
    "    g.ndata[\"x\"] = g.ndata[\"x\"].float().to(device)\n",
    "    edge_norm = torch.ones(g.edata['rel_type'].shape[0]).to(device)\n",
    "    g.edata.update({'norm': edge_norm.view(-1,1).to(device)})\n",
    "    g_list.append(g)\n",
    "    y_list.append(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_phrase(i, phrase_decoder, new_node_embedding, target):\n",
    "    loss = 0\n",
    "    index = target[i]\n",
    "    y_tokenize = torch.Tensor(tokenizer(index2phrase[index])['input_ids'][1:]).view(batch_size,-1).long().to(device)\n",
    "    output = []\n",
    "    phrase_decoder_input = torch.tensor([CLS_token], device=device).view(1, 1)\n",
    "    phrase_decoder_hidden = new_node_embedding\n",
    "    for ni in range(y_tokenize.shape[1]):\n",
    "        phrase_decoder_output, node_decoder_hidden = phrase_generator(\n",
    "            phrase_decoder_input, phrase_decoder_hidden)\n",
    "        topv, topi = phrase_decoder_output.topk(1)\n",
    "        if teacher_forcing:\n",
    "            phrase_decoder_input = y_tokenize[:,ni].view(batch_size, 1)\n",
    "        else:\n",
    "            phrase_decoder_input = topi.squeeze().detach()  # detach from history as input \n",
    "\n",
    "        loss += criterion(phrase_decoder_output.view(batch_size,-1), y_tokenize[:, ni].view(batch_size))\n",
    "        output.append(topi.squeeze().detach().cpu().numpy())\n",
    "    #loss.backward(retain_graph = True)\n",
    "    #node_decoder_optimizer.step()\n",
    "    #graph_encoder_optimizer.step()\n",
    "    #phrase_decoder_optimizer.step()\n",
    "    return loss, tokenizer.decode(output), y_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.5050, device='cuda:0', grad_fn=<AddBackward0>) ['purse [SEP]', 'backpack [SEP]', 'drive [SEP]']\n",
      "tensor(3.5912, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(2.0513, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.6603, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.2918, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.1276, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.1636, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.0973, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.3306, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.0265, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.3955, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.0256, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.0232, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.0531, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.0088, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.0399, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.0109, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.0043, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n",
      "tensor(0.0026, device='cuda:0', grad_fn=<AddBackward0>) ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n"
     ]
    }
   ],
   "source": [
    "#CUDA_LAUNCH_BLOCKING=1\n",
    "learning_rate = 1e-3\n",
    "teacher_forcing = True\n",
    "target = torch.Tensor([[0,0,1]]).to(device).long()\n",
    "\n",
    "#graph_encoder_optimizer = optim.Adam(graph_encoder.parameters(), lr=learning_rate)\n",
    "#node_decoder_optimizer = optim.Adam(node_generator.parameters(), lr=learning_rate)\n",
    "#phrase_decoder_optimizer = optim.Adam(phrase_generator.parameters(), lr=learning_rate)\n",
    "\n",
    "for i in range(200):\n",
    "    for j in range(200):\n",
    "        node_decoder_optimizer.zero_grad()\n",
    "        graph_encoder_optimizer.zero_grad()\n",
    "        phrase_decoder_optimizer.zero_grad()\n",
    "\n",
    "        node_embedding, g_embedding = graph_encoder(g_list[j])\n",
    "        node_decoder_input = torch.tensor([[CLS_token] * batch_size], device=device).view(batch_size, 1)\n",
    "        node_decoder_hidden = (g_embedding.view(1,batch_size,-1), g_embedding.view(1,batch_size,-1))\n",
    "\n",
    "        output = []\n",
    "        phrase_output = []\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = 0\n",
    "        # Generate new nodes\n",
    "        for ni in range(target.shape[1]):\n",
    "            node_decoder_output, node_decoder_hidden = node_generator(\n",
    "                node_decoder_input, node_decoder_hidden)\n",
    "            new_node_embedding = node_decoder_hidden\n",
    "            topv, topi = node_decoder_output.topk(1)\n",
    "            if teacher_forcing:\n",
    "                node_decoder_input = target[:,ni].view(batch_size, 1)\n",
    "            else:\n",
    "                node_decoder_input = topi.squeeze().detach()  # detach from history as input \n",
    "            output.append(topi.squeeze().detach().cpu().numpy())\n",
    "            loss += criterion(node_decoder_output.view(batch_size,-1), target[:, ni].view(batch_size))\n",
    "            phrase_loss, p_output, _ = gen_phrase(ni, phrase_generator, new_node_embedding, y[j])\n",
    "            phrase_output.append(p_output)\n",
    "            loss += phrase_loss\n",
    "\n",
    "        loss.backward(retain_graph = True)\n",
    "        node_decoder_optimizer.step()\n",
    "        graph_encoder_optimizer.step()\n",
    "        phrase_decoder_optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "        print(loss, phrase_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mouse [SEP]', 'heart [SEP]', 'entertain [SEP]'] ['fanatic [SEP]', 'heart [SEP]', 'drink [SEP]']\n",
      "['playroom [SEP]', 'now [SEP]', 'go to war [SEP]'] ['playroom [SEP]', 'wheat [SEP]', 'go to war [SEP]']\n",
      "['can [SEP]', 'it [SEP]', 'copulate [SEP]'] ['can [SEP]', 'it [SEP]', 'copulate [SEP]']\n",
      "['pond [SEP]', 'now [SEP]', 'plate [SEP]'] ['pond [SEP]', 'now [SEP]', 'plate [SEP]']\n",
      "['cook [SEP]', 'exercise [SEP]', 'gambler [SEP]'] ['cook [SEP]', 'exercise [SEP]', 'gambler [SEP]']\n",
      "['choir [SEP]', 'sadness [SEP]', 'travel [SEP]'] ['choir [SEP]', 'sadness [SEP]', 'travel [SEP]']\n",
      "['home [SEP]', 'fin [SEP]', 'fish [SEP]'] ['home [SEP]', 'fin [SEP]', 'fish [SEP]']\n",
      "['sit down [SEP]', 'wash [SEP]', 'this [SEP]'] ['sit down [SEP]', 'wash [SEP]', 'this [SEP]']\n",
      "['shovel [SEP]', 'woman [SEP]', 'place [SEP]'] ['shovel [SEP]', 'woman [SEP]', 'place [SEP]']\n",
      "['fan [SEP]', 'this [SEP]', 'orange [SEP]'] ['fan [SEP]', 'this [SEP]', 'orange [SEP]']\n",
      "['organism [SEP]', 'leaf [SEP]', 'attend rock concert [SEP]'] ['organism [SEP]', 'leaf [SEP]', 'attend rock concert [SEP]']\n",
      "['fruit [SEP]', 'house [SEP]', 'imac [SEP]'] ['fruit [SEP]', 'house [SEP]', 'imac [SEP]']\n",
      "['thief [SEP]', 'watch television [SEP]', 'something [SEP]'] ['thief [SEP]', 'watch television [SEP]', 'something [SEP]']\n",
      "['color [SEP]', 'white [SEP]', 'sport [SEP]'] ['color [SEP]', 'white [SEP]', 'sport [SEP]']\n",
      "['victory [SEP]', 'arm [SEP]', 'assertiveness [SEP]'] ['victory [SEP]', 'arm [SEP]', 'assertiveness [SEP]']\n",
      "['factory [SEP]', 'desk [SEP]', 'universe [SEP]'] ['factory [SEP]', 'desk [SEP]', 'universe [SEP]']\n",
      "['box [SEP]', 'emotion [SEP]', 'spend money [SEP]'] ['box [SEP]', 'emotion [SEP]', 'spend money [SEP]']\n",
      "['playroom [SEP]', 'express yourself [SEP]', 'car [SEP]'] ['playroom [SEP]', 'express yourself [SEP]', 'car [SEP]']\n",
      "['copulate [SEP]', 'alaska [SEP]', 'good [SEP]'] ['copulate [SEP]', 'alaska [SEP]', 'good [SEP]']\n",
      "['mouse [SEP]', 'ink [SEP]', 'entertain [SEP]'] ['mouse [SEP]', 'ink [SEP]', 'entertain [SEP]']\n",
      "['mouse [SEP]', 'cat [SEP]', 'entertain [SEP]'] ['wall [SEP]', 'cat [SEP]', 'blue [SEP]']\n",
      "['australia [SEP]', 'picture frame [SEP]', 'egg [SEP]'] ['australia [SEP]', 'picture frame [SEP]', 'egg [SEP]']\n",
      "['it [SEP]', 'parent [SEP]', 'jar [SEP]'] ['it [SEP]', 'parent [SEP]', 'jar [SEP]']\n",
      "['knife [SEP]', 'store [SEP]', 'toast [SEP]'] ['knife [SEP]', 'store [SEP]', 'toast [SEP]']\n",
      "['music [SEP]', 'internet [SEP]', 'hat [SEP]'] ['music [SEP]', 'internet [SEP]', 'hat [SEP]']\n",
      "['fun [SEP]', 'fear [SEP]', 'picture [SEP]'] ['fun [SEP]', 'fear [SEP]', 'picture [SEP]']\n",
      "['have physical examination [SEP]', 'fun [SEP]', 'criminal [SEP]'] ['have physical examination [SEP]', 'fun [SEP]', 'criminal [SEP]']\n",
      "['neuron [SEP]', 'drink wine [SEP]', 'vehicle [SEP]'] ['neuron [SEP]', 'drink wine [SEP]', 'vehicle [SEP]']\n",
      "['winery [SEP]', 'den [SEP]', 'illness [SEP]'] ['winery [SEP]', 'den [SEP]', 'illness [SEP]']\n",
      "['hear testimony [SEP]', 'aluminium [SEP]', 'lynx [SEP]'] ['hear testimony [SEP]', 'aluminium [SEP]', 'lynx [SEP]']\n",
      "['think [SEP]', 'play game [SEP]', 'hospital [SEP]'] ['think [SEP]', 'play game [SEP]', 'hospital [SEP]']\n",
      "['freeway [SEP]', 'blood [SEP]', 'drive [SEP]'] ['freeway [SEP]', 'blood [SEP]', 'drive [SEP]']\n",
      "['athlete [SEP]', 'water [SEP]', 'clown [SEP]'] ['athlete [SEP]', 'water [SEP]', 'clown [SEP]']\n",
      "['pleasure [SEP]', 'pencil [SEP]', 'action [SEP]'] ['pleasure [SEP]', 'pencil [SEP]', 'action [SEP]']\n",
      "['fan [SEP]', 'work [SEP]', 'ignorance [SEP]'] ['fan [SEP]', 'work [SEP]', 'ignorance [SEP]']\n",
      "['feel [SEP]', 'mountain [SEP]', 'paint picture [SEP]'] ['feel [SEP]', 'mountain [SEP]', 'paint picture [SEP]']\n",
      "['den [SEP]', 'dream [SEP]', 'homeschool [SEP]'] ['den [SEP]', 'dream [SEP]', 'homeschool [SEP]']\n",
      "['north pole [SEP]', 'box [SEP]', 'kingdom [SEP]'] ['north pole [SEP]', 'box [SEP]', 'kingdom [SEP]']\n",
      "['life [SEP]', 'feel ill [SEP]', 'pain [SEP]'] ['life [SEP]', 'feel ill [SEP]', 'pain [SEP]']\n",
      "['write poem [SEP]', 'backpack [SEP]', 'make wine [SEP]'] ['write poem [SEP]', 'backpack [SEP]', 'make wine [SEP]']\n",
      "['news [SEP]', 'picture frame [SEP]', 'entertain [SEP]'] ['news [SEP]', 'picture frame [SEP]', 'stadium [SEP]']\n",
      "['flag [SEP]', 'travel [SEP]', 'animal [SEP]'] ['flag [SEP]', 'travel [SEP]', 'animal [SEP]']\n",
      "['sister [SEP]', 'person [SEP]', 'now [SEP]'] ['sister [SEP]', 'person [SEP]', 'now [SEP]']\n",
      "['this [SEP]', 'cupboard [SEP]', 'nature [SEP]'] ['this [SEP]', 'cupboard [SEP]', 'nature [SEP]']\n",
      "['child [SEP]', 'pleasure [SEP]', 'motel [SEP]'] ['child [SEP]', 'pleasure [SEP]', 'motel [SEP]']\n",
      "['fiddle [SEP]', 'musical instrument [SEP]', 'picture frame [SEP]'] ['fiddle [SEP]', 'musical instrument [SEP]', 'picture frame [SEP]']\n",
      "['plant [SEP]', 'cupboard [SEP]', 'plunger [SEP]'] ['plant [SEP]', 'cupboard [SEP]', 'plunger [SEP]']\n",
      "['have bath [SEP]', 'go fish [SEP]', 'have fun [SEP]'] ['have bath [SEP]', 'go fish [SEP]', 'have fun [SEP]']\n",
      "['sickness [SEP]', 'commit perjury [SEP]', 'kitchen [SEP]'] ['sickness [SEP]', 'commit perjury [SEP]', 'kitchen [SEP]']\n",
      "['gas station [SEP]', 'bookstore [SEP]', 'water vapor [SEP]'] ['gas station [SEP]', 'bookstore [SEP]', 'water vapor [SEP]']\n",
      "['window [SEP]', 'learn [SEP]', 'handbag [SEP]'] ['window [SEP]', 'learn [SEP]', 'handbag [SEP]']\n",
      "['best friend [SEP]', 'ranch [SEP]', 'classroom [SEP]'] ['best friend [SEP]', 'ranch [SEP]', 'classroom [SEP]']\n",
      "['disease [SEP]', 'washington [SEP]', 'boy [SEP]'] ['disease [SEP]', 'washington [SEP]', 'boy [SEP]']\n",
      "['female [SEP]', 'time [SEP]', 'travel [SEP]'] ['female [SEP]', 'time [SEP]', 'travel [SEP]']\n",
      "['girl [SEP]', 'automobile [SEP]', 'lose [SEP]'] ['girl [SEP]', 'automobile [SEP]', 'lose [SEP]']\n",
      "['tear [SEP]', 'art store [SEP]', 'essence [SEP]'] ['tear [SEP]', 'art store [SEP]', 'essence [SEP]']\n",
      "['teach [SEP]', 'alaska [SEP]', 'pain [SEP]'] ['teach [SEP]', 'alaska [SEP]', 'pain [SEP]']\n",
      "['competition [SEP]', 'atheist [SEP]', 'kayak [SEP]'] ['competition [SEP]', 'atheist [SEP]', 'kayak [SEP]']\n",
      "['meditate [SEP]', 'something [SEP]', 'fight war [SEP]'] ['meditate [SEP]', 'something [SEP]', 'fight war [SEP]']\n",
      "['office [SEP]', 'livingroom [SEP]', 'eat pussy [SEP]'] ['office [SEP]', 'livingroom [SEP]', 'eat pussy [SEP]']\n",
      "['sport [SEP]', 'something [SEP]', 'library [SEP]'] ['sport [SEP]', 'something [SEP]', 'library [SEP]']\n",
      "['motor [SEP]', 'flirt [SEP]', 'hatred [SEP]'] ['motor [SEP]', 'flirt [SEP]', 'hatred [SEP]']\n",
      "['woman [SEP]', 'backpack [SEP]', 'mammal [SEP]'] ['woman [SEP]', 'backpack [SEP]', 'mammal [SEP]']\n",
      "['parent [SEP]', 'mail letter [SEP]', 'bread [SEP]'] ['parent [SEP]', 'mail letter [SEP]', 'bread [SEP]']\n",
      "['live [SEP]', 'beer [SEP]', 'take course [SEP]'] ['enjoyment [SEP]', 'beer [SEP]', 'take course [SEP]']\n",
      "['now [SEP]', 'see art [SEP]', 'heavy metal [SEP]'] ['now [SEP]', 'see art [SEP]', 'heavy metal [SEP]']\n",
      "['art [SEP]', 'box [SEP]', 'computer [SEP]'] ['art [SEP]', 'box [SEP]', 'computer [SEP]']\n",
      "['driveway [SEP]', 'brewery [SEP]', 'garage [SEP]'] ['driveway [SEP]', 'brewery [SEP]', 'garage [SEP]']\n",
      "['briefcase [SEP]', 'department store [SEP]', 'chair [SEP]'] ['briefcase [SEP]', 'department store [SEP]', 'chair [SEP]']\n",
      "['doctor office [SEP]', 'winter [SEP]', 'human [SEP]'] ['doctor office [SEP]', 'winter [SEP]', 'human [SEP]']\n",
      "['time [SEP]', 'dummy [SEP]', 'fish [SEP]'] ['time [SEP]', 'dummy [SEP]', 'fish [SEP]']\n",
      "['build [SEP]', 'animal [SEP]', 'it [SEP]'] ['build [SEP]', 'animal [SEP]', 'it [SEP]']\n",
      "['theater [SEP]', 'theatre [SEP]', 'cross street [SEP]'] ['theater [SEP]', 'theatre [SEP]', 'cross street [SEP]']\n",
      "['creativity [SEP]', 'eat meal [SEP]', 'bed [SEP]'] ['creativity [SEP]', 'eat meal [SEP]', 'bed [SEP]']\n",
      "['relax [SEP]', 'entertainment [SEP]', 'mouth [SEP]'] ['relax [SEP]', 'entertainment [SEP]', 'mouth [SEP]']\n",
      "['bill clinton [SEP]', 'shop [SEP]', 'it [SEP]'] ['bill clinton [SEP]', 'shop [SEP]', 'it [SEP]']\n",
      "['fabric [SEP]', 'sunburn [SEP]', 'dish [SEP]'] ['fabric [SEP]', 'sunburn [SEP]', 'dish [SEP]']\n",
      "['study [SEP]', 'bird [SEP]', 'time [SEP]'] ['study [SEP]', 'bird [SEP]', 'time [SEP]']\n",
      "['enemy [SEP]', 'now [SEP]', 'corn [SEP]'] ['enemy [SEP]', 'now [SEP]', 'corn [SEP]']\n",
      "['someone [SEP]', 'swim [SEP]', 'vineyard [SEP]'] ['someone [SEP]', 'swim [SEP]', 'vineyard [SEP]']\n",
      "['wood [SEP]', 'classroom [SEP]', 'wait [SEP]'] ['wood [SEP]', 'classroom [SEP]', 'wait [SEP]']\n",
      "['map [SEP]', 'thin [SEP]', 'pollution [SEP]'] ['map [SEP]', 'thin [SEP]', 'pollution [SEP]']\n",
      "['ball [SEP]', 'water [SEP]', 'human body [SEP]'] ['ball [SEP]', 'water [SEP]', 'human body [SEP]']\n",
      "['party [SEP]', 'compete [SEP]', 'yoyo [SEP]'] ['party [SEP]', 'compete [SEP]', 'yoyo [SEP]']\n",
      "['compute [SEP]', 'poop [SEP]', 'entrance hall [SEP]'] ['compute [SEP]', 'poop [SEP]', 'entrance hall [SEP]']\n",
      "['fruit [SEP]', 'star [SEP]', 'create art [SEP]'] ['fruit [SEP]', 'star [SEP]', 'create art [SEP]']\n",
      "['ink [SEP]', 'need food [SEP]', 'fight [SEP]'] ['ink [SEP]', 'need food [SEP]', 'fight [SEP]']\n",
      "['rule [SEP]', 'hand [SEP]', 'exercise [SEP]'] ['rule [SEP]', 'hand [SEP]', 'exercise [SEP]']\n",
      "['play baseball [SEP]', 'chair [SEP]', 'coat hanger [SEP]'] ['play baseball [SEP]', 'chair [SEP]', 'coat hanger [SEP]']\n",
      "['take course [SEP]', 'pleasure [SEP]', 'bordom [SEP]'] ['take course [SEP]', 'pleasure [SEP]', 'bordom [SEP]']\n",
      "['companionship [SEP]', 'classroom [SEP]', 'ink [SEP]'] ['companionship [SEP]', 'classroom [SEP]', 'ink [SEP]']\n",
      "['medal [SEP]', 'art store [SEP]', 'oxygen [SEP]'] ['medal [SEP]', 'art store [SEP]', 'oxygen [SEP]']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cook food [SEP]', 'erection [SEP]', 'drink [SEP]'] ['cook food [SEP]', 'erection [SEP]', 'drink [SEP]']\n",
      "['fly [SEP]', 'customer [SEP]', 'visit friend [SEP]'] ['fly [SEP]', 'customer [SEP]', 'visit friend [SEP]']\n",
      "['this [SEP]', 'stall [SEP]', 'farmhouse [SEP]'] ['this [SEP]', 'stall [SEP]', 'farmhouse [SEP]']\n",
      "['cry [SEP]', 'storage [SEP]', 'this [SEP]'] ['cry [SEP]', 'storage [SEP]', 'this [SEP]']\n",
      "['use computer [SEP]', 'bake bread [SEP]', 'cow [SEP]'] ['use computer [SEP]', 'bake bread [SEP]', 'cow [SEP]']\n",
      "['wool [SEP]', 'library [SEP]', 'alarm clock [SEP]'] ['wool [SEP]', 'library [SEP]', 'alarm clock [SEP]']\n",
      "['kill person [SEP]', 'muscle [SEP]', 'bullet [SEP]'] ['kill person [SEP]', 'muscle [SEP]', 'bullet [SEP]']\n",
      "['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]'] ['scientist [SEP]', 'baby [SEP]', 'tooth [SEP]']\n"
     ]
    }
   ],
   "source": [
    "g_list = []\n",
    "y_list = []\n",
    "for i in range(200):\n",
    "    g = sub_graph_G.subgraph(X[i])\n",
    "    g.copy_from_parent()\n",
    "    g.ndata[\"x\"] = g.ndata[\"x\"].float().to(device)\n",
    "    edge_norm = torch.ones(g.edata['rel_type'].shape[0]).to(device)\n",
    "    g.edata.update({'norm': edge_norm.view(-1,1).to(device)})\n",
    "    g_list.append(g)\n",
    "    y_list.append(y[i])\n",
    "\n",
    "for j in range(100,200):\n",
    "    node_embedding, g_embedding = graph_encoder(g_list[j])\n",
    "    node_decoder_input = torch.tensor([[CLS_token] * batch_size], device=device).view(batch_size, 1)\n",
    "    node_decoder_hidden = (g_embedding.view(1,batch_size,-1), g_embedding.view(1,batch_size,-1))\n",
    "\n",
    "    output = []\n",
    "    phrase_output = []\n",
    "    y_token = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Generate new nodes\n",
    "    for ni in range(target.shape[1]):\n",
    "        node_decoder_output, node_decoder_hidden = node_generator(\n",
    "            node_decoder_input, node_decoder_hidden)\n",
    "        new_node_embedding = node_decoder_hidden\n",
    "        topv, topi = node_decoder_output.topk(1)\n",
    "        if teacher_forcing:\n",
    "            node_decoder_input = target[:,ni].view(batch_size, 1)\n",
    "        else:\n",
    "            node_decoder_input = topi.squeeze().detach()  # detach from history as input \n",
    "        output.append(topi.squeeze().detach().cpu().numpy())\n",
    "        phrase_loss, p_output, y_tokenize = gen_phrase(ni, phrase_generator, new_node_embedding, y[j])\n",
    "        phrase_output.append(p_output)\n",
    "        y_token.append(tokenizer.decode(y_tokenize.view(-1)))\n",
    "    print(phrase_output, y_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3388"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index2phrase[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
